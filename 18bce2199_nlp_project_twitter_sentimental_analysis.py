# -*- coding: utf-8 -*-
"""18BCE2199_NLP_PROJECT_twitter sentimental analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqjzdO4vauOjmA9yXOTmgT1RnkP4B55S
"""

pip install -U textblob

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import nltk
nltk.download('wordnet')
data = pd.read_csv('text_emotion.csv')

data = data.drop('author', axis=1)
data

#Dropping rows with other emotion labels

data  = data.drop(data[data.sentiment == 'anger'].index)
data  = data.drop(data[data.sentiment == 'boredom'].index)
data  = data.drop(data[data.sentiment == 'enthusiasm'].index)
data  = data.drop(data[data.sentiment == 'empty'].index)
data  = data.drop(data[data.sentiment == 'fun'].index)
data  = data.drop(data[data.sentiment == 'relief'].index)
data  = data.drop(data[data.sentiment == 'surprise'].index)
data  = data.drop(data[data.sentiment == 'love'].index)
data  = data.drop(data[data.sentiment == 'hate'].index)
data  = data.drop(data[data.sentiment == 'neutral'].index)
data  = data.drop(data[data.sentiment == 'worry'].index)

# Commented out IPython magic to ensure Python compatibility.
# --- run once if needed ---
# %pip install -q emoji==2.10.1 contractions nltk

import re, nltk, emoji, contractions
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

# --- NLTK resources (safe if run multiple times) ---
for corp in ['stopwords','wordnet','omw-1.4']:
    try: nltk.data.find(f'corpora/{corp}')
    except LookupError: nltk.download(corp)

# tagger names differ across NLTK versions; try both
for tagger in ['averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng']:
    try: nltk.data.find(f'taggers/{tagger}')
    except LookupError:
        try: nltk.download(tagger)
        except: pass

# --- helpers ---
NEGATIONS = {
    'no','nor','not',"don't","didn't","isn't","wasn't","aren't","weren't",
    "won't","can't","couldn't","shouldn't","wouldn't","doesn't","ain't","n't"
}
STOP = set(stopwords.words('english')) - NEGATIONS
WN = WordNetLemmatizer()

def _map_pos(tag):
    return {'J':'a','V':'v','N':'n','R':'r'}.get(tag[:1], 'n')

def clean_tweet(text, lemmatize=True):
    if not isinstance(text, str):
        return ''
    x = text.lower()

    # Twitter-specific
    x = re.sub(r'http\S+|www\.\S+', ' URL ', x)     # URLs -> token
    x = re.sub(r'@\w+', ' USER ', x)                # mentions -> token
    x = re.sub(r'\brt\b', ' ', x)                   # strip retweets
    x = re.sub(r'#(\w+)', r'\1', x)                 # keep hashtag word

    # Emojis / contractions / elongations / numbers
    x = emoji.demojize(x)                           # ðŸ™‚ -> :slightly_smiling_face:
    x = contractions.fix(x)                         # can't -> can not
    x = re.sub(r'(.)\1{2,}', r'\1\1', x)            # soooo -> soo
    x = re.sub(r'\d+', ' NUM ', x)                  # numbers -> token

    # Punctuation & whitespace
    x = re.sub(r'[^\w\s]', ' ', x)                  # keep letters/numbers/underscore
    x = re.sub(r'\s+', ' ', x).strip()

    # Lemmatize (optional; set lemmatize=False if slow)
    if lemmatize and x:
        toks = x.split()
        tags = pos_tag(toks)
        x = ' '.join(WN.lemmatize(w, _map_pos(t)) for w, t in tags)

    # Stopwords (keep negations)
    toks = [w for w in x.split() if w not in STOP]
    return ' '.join(toks)

# ---- APPLY HERE ----
data['content_clean'] = data['content'].apply(clean_tweet)

# (Optional) quick sanity check
print(data[['content','content_clean']].head(3))

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Lowercasing
data['content'] = data['content'].str.lower()

# Removing punctuation/symbols
data['content'] = data['content'].str.replace(r'[^\w\s]', ' ', regex=True)

# Removing stopwords
stop = set(stopwords.words('english'))
data['content'] = data['content'].apply(lambda x: " ".join(word for word in x.split() if word not in stop))

# Code to find the top 10,000 rarest words (modify according to your dataset)
# appearing in the data
freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]

# Removing all those rarely appearing words from the data
freq = list(freq.index)
data['content'] = data['content'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))

#Encoding output labels 'sadness' as '1' & 'happiness' as '0'
from sklearn import preprocessing
lbl_enc = preprocessing.LabelEncoder()
y = lbl_enc.fit_transform(data.sentiment.values)

# Splitting into training and testing data in 90:10 ratio
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)
X_train

# Extracting TF-IDF parameters
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))
X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf = tfidf.fit_transform(X_val)

# Extracting Count Vectors Parameters
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(analyzer='word')
count_vect.fit(data['content'])
X_train_count =  count_vect.transform(X_train)
X_val_count =  count_vect.transform(X_val)

"""# Models using the TF-IDF features

## Model 1: Multinomial Naive Bayes Classifier
"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import MultinomialNB
accuracy=[]
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
y_pred = nb.predict(X_val_tfidf)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 2: Linear SVM"""

from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)
lsvm.fit(X_train_tfidf, y_train)
y_pred = lsvm.predict(X_val_tfidf)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('svm using tfidf accuracy %s' %accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 3: logistic regression"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=1)
logreg.fit(X_train_tfidf, y_train)
y_pred = logreg.predict(X_val_tfidf)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 4: Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train_tfidf, y_train)
y_pred = rf.predict(X_val_tfidf)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('randon forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""# Models using count vectors features

## Model 1: Multinomial Naive Bayes Classifier
"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_count, y_train)
y_pred = nb.predict(X_val_count)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 2: Linear SVM"""

from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)
lsvm.fit(X_train_count, y_train)
y_pred = lsvm.predict(X_val_count)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 3: Logistic Regression"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=1)
logreg.fit(X_train_count, y_train)
y_pred = logreg.predict(X_val_count)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""## Model 4: Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train_count, y_train)
y_pred = rf.predict(X_val_count)
accuracy.append(accuracy_score(y_pred, y_val)*100)
print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))
confusion_matrix(y_val, y_pred)

"""# Graph based analysis"""

objects = ('Naive Bayes tfidf', 'Svm using tfidf',
           'Logistic regression tf idf', 'Random forest tfid',
           'Naive Bayes Count Vector', 'Svm using Count Vector',
           'Logistic regression Count Vector', 'Random forest Count Vector')
y_pos = np.arange(len (objects))
plt.barh(y_pos, accuracy, align='center', alpha=0.5)
plt.yticks (y_pos, objects)
plt.xlabel('Accuracy')
plt.title('Accuracy of each model')

plt.show()

"""# REALITY CHECK"""

tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',
'Things are looking great. It was such a good day',
'Success is right around the corner. Lets celebrate this victory',
'Everything is more beautiful when you experience them with a smile!',
'Now this is my worst, okay? But I am gonna get better.',
'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',
'This is quite depressing. I am filled with sorrow',
'His death broke my heart. It was a sad day'])

# Doing some preprocessing on these tweets as done before
tweets[0] = tweets[0].str.replace('[^\w\s]',' ')
from nltk.corpus import stopwords
stop = stopwords.words('english')
tweets[0] = tweets[0].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
from textblob import Word
tweets[0] = tweets[0].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

# Extracting Count Vectors feature from our tweets
tweet_count = count_vect.transform(tweets[0])

#Predicting the emotion of the tweet using our already trained linear SVM
tweet_pred = lsvm.predict(tweet_count)

print(tweet_pred)

tweets = pd.DataFrame({
    'text': [
        'I am very happy today! The atmosphere looks cheerful',
        'This is quite depressing. I am filled with sorrow'
    ]
})

# clean with the SAME function
tweets['clean'] = tweets['text'].apply(clean_tweet)

# transform with the SAME vectorizer used to train the model
X_tweets = tfidf.transform(tweets['clean'])

# use the model that was trained on TF-IDF (change to your best model)
best_model = LogisticRegression(C=1.0, max_iter=1000, class_weight='balanced', solver='liblinear')
best_model.fit(X_train_tfidf, y_train)
pred = best_model.predict(X_tweets)
tweets['predicted_sentiment'] = lbl_enc.inverse_transform(pred)
print(tweets[['text','predicted_sentiment']])

# FINALIZE: train/val/test + metrics + interpretability + save
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score
import numpy as np, joblib

# Data (clean text + encoded labels)
X_all = data['content_clean'].values
y_all = lbl_enc.transform(data['sentiment'].values)

# 80/10/10 split: train/val/test (stratified, reproducible)
X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=42 )
X_val, X_test, y_val, y_test = train_test_split( X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42 )

# Pipeline + small grid (fast, strong baseline)
pipe = Pipeline([
    ("tfidf", TfidfVectorizer(sublinear_tf=True)),
    ("clf", LogisticRegression(max_iter=1000, class_weight="balanced", solver="liblinear", random_state=42)),
])

param_grid = {
    "tfidf__ngram_range": [(1,1), (1,2)],
    "tfidf__min_df": [2, 5],
    "tfidf__max_df": [0.95, 0.98],
    "clf__C": [0.5, 1.0, 2.0],
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param_grid, scoring="f1_macro", cv=cv, n_jobs=-1)
gs.fit(X_train, y_train)

# Validation quick check
val_pred = gs.predict(X_val)
print("Val macro-F1:", f1_score(y_val, val_pred, average="macro"))

# ---- Final TEST report ----
y_pred = gs.predict(X_test)
print("\n=== TEST REPORT ===")
print(classification_report(y_test, y_pred, target_names=lbl_enc.classes_))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
if len(lbl_enc.classes_) == 2:
    y_proba = gs.predict_proba(X_test)[:, 1]
    print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# ---- Top features (LogReg) ----
tfidf = gs.best_estimator_["tfidf"]
clf   = gs.best_estimator_["clf"]
feat  = tfidf.get_feature_names_out()
coef  = clf.coef_[0]
top_pos = np.argsort(coef)[-15:][::-1]
top_neg = np.argsort(coef)[:15]
print("\nTop features for class:", lbl_enc.classes_[np.argmax(clf.classes_)], feat[top_pos])
print("Top features for class:", lbl_enc.classes_[np.argmin(clf.classes_)], feat[top_neg])

# ---- Save best model ----
joblib.dump(gs.best_estimator_, "twitter_sentiment_pipeline.joblib")
print("\nSaved model -> twitter_sentiment_pipeline.joblib")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize

def _score_vector(model, X):
    # Works for pipelines or plain estimators
    if hasattr(model, "predict_proba"):
        S = model.predict_proba(X)
        return S
    if hasattr(model, "decision_function"):
        S = model.decision_function(X)
        # decision_function may return shape (n_samples,) for binary;
        # convert to (n_samples, 2) like proba for uniform handling
        if S.ndim == 1:
            S = np.c_[1 - (S - S.min()) / (S.max() - S.min() + 1e-12),
                      (S - S.min()) / (S.max() - S.min() + 1e-12)]
        return S
    # Fallback (not ideal for ROC): use predictions as scores
    P = model.predict(X)
    # make a 2-col "score" guess
    return np.c_[1 - P, P]

def plot_roc(model, X, y, lbl_enc=None, positive_class_name=None, title="ROC Curve"):
    # Determine classes and binarize y if needed
    classes = np.unique(y)
    is_binary = len(classes) == 2

    # Scores: shape (n_samples, n_classes) if possible
    S = _score_vector(model, X)

    plt.figure()
    if is_binary:
        # pick positive label
        if positive_class_name is not None and lbl_enc is not None:
            pos_label = lbl_enc.transform([positive_class_name])[0]
        else:
            # default: label "1" is positive (LabelEncoder likely mapped 'sadness' to 1)
            pos_label = 1

        # Get score column for pos class
        # If S is 1D, make it a 2D proba-like array above; else select the correct column
        if S.ndim == 1:
            pos_scores = S
        else:
            # find index of pos_label among model.classes_ if available; else assume column 1
            if hasattr(model, "classes_"):
                # handle pipeline: try to get final estimator
                clf = model
                if hasattr(model, "named_steps") and "clf" in model.named_steps:
                    clf = model.named_steps["clf"]
                if hasattr(clf, "classes_"):
                    class_order = clf.classes_
                    # find column idx of pos_label
                    try:
                        col = list(class_order).index(pos_label)
                    except ValueError:
                        col = 1  # fallback
                    pos_scores = S[:, col]
                else:
                    pos_scores = S[:, 1] if S.shape[1] > 1 else S.ravel()
            else:
                pos_scores = S[:, 1] if S.shape[1] > 1 else S.ravel()

        fpr, tpr, _ = roc_curve(y, pos_scores, pos_label=pos_label)
        auc_val = roc_auc_score(y, pos_scores)
        plt.plot(fpr, tpr, label=f"AUC = {auc_val:.3f}")
    else:
        # One-vs-rest micro-average
        Y = label_binarize(y, classes=classes)
        # If S is binary scores, convert to one score per class
        if S.ndim == 1:
            S = np.c_[1 - S, S]
        fpr, tpr, _ = roc_curve(Y.ravel(), S.ravel())
        auc_val = roc_auc_score(Y, S, average="micro", multi_class="ovr")
        plt.plot(fpr, tpr, label=f"micro-avg AUC = {auc_val:.3f}")

    plt.plot([0, 1], [0, 1], linestyle="--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

# ------- USE IT -------
# If you used GridSearchCV:
model = gs.best_estimator_   # or: model = your_fitted_pipeline_or_estimator
X_eval = X_test              # or X_val
y_eval = y_test              # or y_val

# If you want "happiness" as the positive class on the ROC:
# plot_roc(model, X_eval, y_eval, lbl_enc=lbl_enc, positive_class_name="happiness")

# Default (positive label = 1, likely 'sadness'):
plot_roc(model, X_eval, y_eval, lbl_enc=lbl_enc, title="ROC â€” Test Set")